# revnet-zero
A high-performance library of reversible transformer layers with on-the-fly activation recomputation, cutting GPU memory usage by >70% during 256k-token pre-training. Implements cutting-edge techniques from the 2024 energy-efficient reversible-attention paper, enabling massive context windows on consumer hardware.
